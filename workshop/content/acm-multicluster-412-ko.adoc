= Kubernetes를 위한 고급 클러스터 관리 - RHACM

== 클러스터 및 클러스터 라이프사이클 작업

클러스터 라이프사이클 관리란 멀티 클라우드 환경에서 클러스터를 생성, 업그레이드, 삭제 및 가져오는 작업들을 말합니다.

아래 주소를 통해 OpenShift 웹콘솔로 이동하세요: 

{{ MASTER_URL }}

`Local-cluster`가 선택되어있는 경우 `All Clusters`를 선택하면 ACM 콘솔화면이 표시됩니다.

image::images/acm-images/RHACM_all_cluster.png[]


[start=1]
1. ACM에 로그인한 후, *Credentials* 메뉴로 이동한 다음 *Add Credentials*를 선택합니다.

image::images/acm-images/credentials.png[]

image::images/acm-images/add_cred.png[]

=== 클라우드 공급자에 대한 정보를 제공해야 합니다. :

* 클라우드 공급자 자격 증명 *Amazon Web Services* 선택 +

image::images/acm-images/aws_cred.png[]

### 화면 1: 기본 정보

* Credential Name:  `aws`
* Namespace: `open-cluster-management`
* Base DNS Domain:  This information is located on the demo console under the field *rhacm_aws_subdomain*. You can also find it on the creds page for this workshop under *Top level domain*.

이 정보는 데모 콘솔의 *rhacm_aws_subdomain* 필드 아래에 있습니다. 

이 정보를 복사하여 붙여넣을 때 URL에서 점을 생략했는지 확인하세요. 예를 들어 `.sandbox1536.opentlc.com`이 아니라 `sandbox1536.opentlc.com`이어야 합니다.


image::images/acm-images/cred_basicinfo.png[]

[start=2]
2. Click NEXT

### 화면 2 Amazon Web Services

* Access Key ID: This information is located on the creds page for the 
demo console under the field *AWS_ACCESS_KEY_ID*

* Secret Access Key ID: This information is located on the creds page 
for the demo console under the field *AWS_SECRET_ACCESS_KEY*

image::images/acm-images/screen2.png[]

[start=3]
3. Click NEXT - We don't need to configure a Proxy so we can skip this screen

* Click NEXT



### Screen 4 Pull secret and SSH

* Red Hat OpenShift pull secret:  https://cloud.redhat.com/openshift/install/pull-secret[Get the pull secret from cloud.redhat.com - RH login Required]

image::images/acm-images/pull_secret.png[]

* SSH private and public keys:  Use an existing key pair or https://docs.openshift.com/container-platform/4.9/installing/installing_aws/installing-aws-default.html#ssh-agent-using_installing-aws-default[generate a new ssh key]

> Please note you might need *console.redhat.com* access to get these keys

[start=4]
4. Click NEXT

image::images/acm-images/screen4.png[]

[start=5]
5. Verify the information and click *ADD*

== AWS에서 새 OpenShift 클러스터 생성


1. 메뉴에서 *Infrastructure → Clusters*를 선택합니다.
2. *Create Cluster*를 클릭하세요.

image::images/acm-images/create_cluster.png[]


[start=3]
3. *Amazon Web services* 선택

image::images/acm-images/aws_cluster.png[]

[start=4]
4. *Standalone* 선택

image::images/acm-images/standalone.png[]

[start=5]
5. 아까 생성한 *AWS*라는 *Infrastructure provider credential*을 선택합니다.

* 원하는 *cluster name*을 추가합니다.
* 지금은 *Cluster set*를 비워 두세요.
* *Release Image*를 선택하고 4.12.26 버전을 선택하세요.
* *environment=prod*라는 추가 라벨을 추가합니다.
* NEXT 클릭

*아래 표*를 참고하여 region을 변경하세요.

|===
|귀하의 위치 | 선택할 AWS 리전
|*NORTH AMERICA*|Select *us-west-1* or *us-west-2*

|*EUROPE / EMEA*|Select *eu-west-2* or *eu-west-3*
|*ASIA PACIFIC*|Select *ap-southeast-2* or *ap-northeast-2* or *ap-east-1*
|===

[start=6]
6. Click NEXT on the other screens or select *7 - Review* on the menu and then click *CREATE*

_이 과정은 이 과정을 수강하는 시점의 AWS 트래픽에 따라 약 30~40분 정도 소요됩니다._

== AWS에서 단일 노드 클러스터(SNO) 생성

기본 클러스터가 프로비저닝될 때까지 기다리는 동안 계속해서 단일 노드 클러스터를 프로비저닝하겠습니다. 여기서는 테스트용 클러스터를 구축할 때, 시간과 리소스를 절약하기 위해 단일 노드 클러스터(OCP 4.8 이상 필요)를 생성하는 방법을 보여줍니다.

*주의* 퍼블릭 클라우드에서 SNO 클러스터를 프로비저닝하는 것은 현재 지원되지 않으며, 베어메탈로만 가능합니다. 아래 예에서는 퍼블릭 클라우드를 활용하여 기능만 보여줍니다.

1. From the menu select *Infrastructure → Clusters*
2. Click *Create Cluster*
3. Select *Amazon Web services*
4. Select *Standalone*

[start=5]
5. Please set it up as follows:
* Select the *Infrastructure provider credential*  *AWS*
* Click NEXT
* Add the desired cluster name. Leave the Cluster set empty for now
* Select a *Release Image*, select a *OCP 4.12.26 version*
* Add an Additional label of *environment=qa*
* Click NEXT
* Change the region to *see table below*


|===
|Your Location | AWS Region to select
|*NORTH AMERICA*|Select *us-west-1* or *us-west-2*

|*EUROPE / EMEA*|Select *eu-west-2* or *eu-west-3*
|*ASIA PACIFIC*|Select *ap-southeast-2* or *ap-northeast-2* or *ap-east-1*
|===

[start=6]
6. Expand the *Worker Pools*, and change the worker node count to 0

image::images/acm-images/node_0.png[]

[start=7]
7. Click on step 7 to Review *before* proceeding, turn *YAML: ON at the top of the screen.*

[start=8]
8. Click on the *install-config* tab in the YAML window pane and *change the master replica number to 1* (will likely be 3).  Double check that the worker replica is 0.

image::images/acm-images/install-config.png[]

[start=9]
9. Click back on the *cluster* tab in the YAML window pane and locate the section that defines an object of type: *kind: MachinePool*. Add the following line at the end of the *MachinePool* section.
----
  skipMachinePools: true
----

It should look something like this:
----
apiVersion: hive.openshift.io/v1
kind: MachinePool
metadata:
  name: cluster2-worker
  namespace: 'cluster2'
spec:
  clusterDeploymentRef:
    name: 'cluster2'
  name: worker
  platform:
    aws:
      rootVolume:
        iops: 2000
        size: 100
        type: io1
      type: m5.xlarge
  replicas: 0
  skipMachinePools: true
----

Be sure the new line is at the same indentation as the previous line.

[start=10]
10. Click on “*Create*” and the single node cluster creation will go through.

_This process takes about 10 to 20 minutes depending on AWS traffic at the time this course is taken. Make sure you monitor for any failures and address as needed_


== Creating and Managing Applications with Red Hat Advanced Cluster Management For Kubernetes


In the previous lab, you explored the Cluster Lifecycle functionality in RHACM. This allowed you to create new OpenShift® clusters, which you can now use to deploy applications.

Application Lifecycle functionality in RHACM provides the processes that are used to manage application resources on your managed clusters. This allows you to define a single or multi-cluster application using Kubernetes specifications, but with additional automation of the deployment and lifecycle management of resources to individual clusters. An application designed to run on a single cluster is straightforward and something you ought to be familiar with from working with OpenShift fundamentals. A multi-cluster application allows you to orchestrate the deployment of these same resources to multiple clusters, based on a set of rules you define for which clusters run the application components.

This table describes the different components that the Application Lifecycle model in RHACM is composed of:



|===
|*Resource*|*Purpose *

|Channel|Defines a place where deployable resources are stored, such as an object store, Kubernetes namespace, Helm repository, or GitHub repository.
|Subscription|Definitions that identify deployable resources available in a Channel resource that are to be deployed to a target cluster.
|PlacementRule|Defines the target clusters where subscriptions deploy and maintain the application. It is composed of Kubernetes resources identified by the Subscription resource and pulled from the location defined in the Channel resource.
|Application|A way to group the components here into a more easily viewable single resource. An Application resource typically references a Subscription resource.
|===


These are all Kubernetes custom resources, defined by a Custom Resource Definition (CRD), that are created for you when RHACM is installed. By creating these as Kubernetes native objects, you can interact with them the same way you would with a Pod. For instance, running +oc get application+ retrieves a list of deployed RHACM applications just as +oc get pods+ retrieves a list of deployed Pods.

This may seem like a lot of extra resources to manage in addition to the deployables that actually make up your application. However, they make it possible to automate the composition, placement, and overall control of your applications when you are deploying to many clusters. With a single cluster, it is easy to log in and run +oc create -f…​.+ If you need to do that on a dozen clusters, you want to make sure you do not make a mistake or miss a cluster, and you need a way to schedule and orchestrate updates to your applications. Leveraging the Application Lifecycle Builder in RHACM allows you to easily manage multi-cluster applications.

== Creating an Application


Prerequisites:

* Navigate to *Infrastructure → Clusters*
* Click on the *local-cluster*
* Click the *edit* button under *Labels* and add a *label* : `environment=dev`
* Verify the new clusters you build have the correct labels, it should be as follows:
** *Local-Cluster* - `environment=dev`
** *AWS 1st Cluster* - `environment=prod`
** *AWS 2nd Cluster* - `environment=qa`

image::images/acm-images/env_labels.png[]

[start=1]
1. Navigate to *Applications*
2. Click *Create application, select Subscription*. 

image::images/acm-images/create_sub_app.png[]

[start=3]
3. Enter the following information:

** *Name*: `book-import`
** *Namespace*: `book-import`
** Under repository types, select the *GIT* repository
** *URL:*  https://github.com/hichammourad/book-import.git[https://github.com/hichammourad/book-import.git]
** *Branch*:  `master-no-pre-post`
** *Path:*  `book-import`

[start=4]
4. Verify that *Deploy application resources only on clusters matching specified labels* is selected and enter the following information
** *environment*: `dev`

image::images/acm-images/label_sub.png[]


You do not need the to include the colon (:)

[start=5]
5. Verify all the information is correct. Click *Create*

It will take a few minutes to deploy the application, *Click* on the *Topology* view and verify that *all of the check marks are green*.

image::images/acm-images/book_topology.png[]

[start=6]
6. Under the topology view, Select the *Route* and click on the *Launch Route* *URL*, this will take you to the Book Import application with several books available.

image::images/acm-images/book_website.png[]

Feel free to experiment with the application.  Edit it and change the label to `environment=prod`.  What happens to the application?

You have now completed the overview of the *Application Lifecycle functionality in RHACM.*

You successfully deployed an application to a target cluster using RHACM. This approach leveraged a Git repository which housed all of the manifests that defined your application. RHACM was able to take those manifests and use them as deployables, which were then deployed to the target cluster.

You also leverage the power of labels and deploy the application to your imported cluster. I highly encourage you to play around with the labels and deploy this application to your local cluster. You can also create other clusters and or applications if you so desire.

== Governance, Risk, and Compliance (Security and compliance use case)

=== Creating Policies in ACM


At this point, you have completed the overview labs for Cluster Lifecycle and Application Lifecycle capabilities in RHACM. In the Cluster Lifecycle Lab, you learned how RHACM can help manage the lifecycles of your Kubernetes clusters, including both deploying new clusters and importing existing clusters. In that lab, you created new clsters and used your RHACM instance to manage them.

In the Application Lifecycle Lab, you continued exploring RHACM functionality and learned how to deploy and configure an application. You used the cluster that you added in the first module as the target for deploying an application.

Now that you have a cluster and a deployed application, you need to make sure that they do not drift from their original configurations. This kind of drift is a serious problem, because it can happen from benign and benevolent fixes and changes, as well as malicious activities that you might not notice but can cause significant problems. The solution that RHACM provides for this is the Governance, Risk, and Compliance, or GRC, functionality.

==== Review GRC Functionality

To begin, it is important to define exactly what GRC is. In RHACM, you build policies that are applied to managed clusters. These policies can do different things, which are described below, but they ultimately serve to govern the configurations of your clusters. This governance over your cluster configurations reduces risk and ensures compliance with standards defined by stakeholders, which can include security teams and operations teams

This table describes the three types of policy controllers available in RHACM along with the remediation mode they support:

|===
|*Policy Controller*|*Purpose*|*Enforce or Inform*

|Configuration|Used to configure any Kubernetes resource across your clusters. Where these resources are created or configured is determined by the namespaces you include (or exclude) in the policy.|Both
|Certificate|Used to detect certificates that are close to expiring. You can configure the certificate policy controller by updating the minimum duration parameter in your controller policy. When a certificate expires in less than the minimum duration, the policy becomes noncompliant. Certificates are identified from secrets in the included namespaces.|Inform
|Identity and Access Management (IAM)|Used to receive notifications about IAM policies that are noncompliant. In the 1.0 version of RHACM, this checks for compliance with the number of cluster administrators you allow in your cluster.    |inform
|===

You need to create three different resources in order to implement the policy controllers:

|===
|*Resource*|*Function*

|Policy|The Policy defines what you actually want to check and possibly configure (with enforce). Policies include a policy-template which defines a list of objectDefinitions. The policy also determines the namespaces it is applied to, as well as the remediation actions it takes.
|Placement Rule|Identifies a list of managed clusters that are targeted when using this PlacementRule.
|PlacementBinding|Connect the policy to the PlacementRule.
|===


This is a complex topic, and this course is only providing an overview. Please consult the https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.5/html-single/governance/index#governance[GRC product documentation] for more details on any of these policy controllers.

1. Navigate to the *Governance* screen and click *create policy.*
2. Navigate to the https://github.com/stolostron/policy-collection/tree/main/stable/CM-Configuration-Management[GitHub Repo] with all the policies and select the https://github.com/stolostron/policy-collection/blob/main/stable/SC-System-and-Communications-Protection/policy-etcdencryption.yaml[Etcd Encryption]
3. On the *ETCD Encryption Policy* click the *RAW* button on the policy.
4. Copy the raw YAML.
5. Under the *Create Policy* screen, enable the *YAML*. Copy and Paste the *RAW YAML* from the GitHub Repo

It should look something like this:

----
apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  name: policy-etcdencryption
  annotations:
    policy.open-cluster-management.io/standards: NIST SP 800-53
    policy.open-cluster-management.io/categories: SC System and Communications Protection
    policy.open-cluster-management.io/controls: SC-28 Protection Of Information At Rest
spec:
  remediationAction: inform
  disabled: false
  policy-templates:
    - objectDefinition:
        apiVersion: policy.open-cluster-management.io/v1
        kind: ConfigurationPolicy
        metadata:
          name: enable-etcd-encryption
        spec:
          remediationAction: inform
          severity: low
          object-templates:
            - complianceType: musthave
              objectDefinition:
                apiVersion: config.openshift.io/v1
                kind: APIServer
                metadata:
                  name: cluster
                spec:
                  encryption:
                    type: aescbc
    - objectDefinition:
        apiVersion: policy.open-cluster-management.io/v1
        kind: ConfigurationPolicy
        metadata:
          name: enable-etcd-encryption-status-kubeapi
        spec:
          remediationAction: inform
          severity: low
          object-templates:
            - complianceType: musthave
              objectDefinition:
                apiVersion: operator.openshift.io/v1
                kind: KubeAPIServer
                metadata:
                  name: cluster
                status:
                  conditions:
                    - message: 'All resources encrypted: secrets, configmaps'
                      reason: EncryptionCompleted
---
apiVersion: policy.open-cluster-management.io/v1
kind: PlacementBinding
metadata:
  name: binding-policy-etcdencryption
placementRef:
  name: placement-policy-etcdencryption
  kind: PlacementRule
  apiGroup: apps.open-cluster-management.io
subjects:
- name: policy-etcdencryption
  kind: Policy
  apiGroup: policy.open-cluster-management.io
---
apiVersion: apps.open-cluster-management.io/v1
kind: PlacementRule
metadata:
  name: placement-policy-etcdencryption
spec:
  clusterConditions:
  - status: "True"
    type: ManagedClusterConditionAvailable
  clusterSelector:
    matchExpressions:
      - {key: environment, operator: In, values: ["dev"]}
----


[start=6]
6. Continue filling out the infromation:

* *Namespace*: `default`
* Click on Step 5 and verify that everything looks correct.
* Click Submit.

[start=7]
7. Navigate to the Results screen, allow the scan to complete, it shouldn't take more than 3 minutes.

Once complete notice the violations you have, since we created this policy as a Inform only it will not fix any of the violations, lets go ahead and fix them

[start=8]
8. On the top of the policy click on the *Actions → Edit Policy*
9. Select *Step 2* and change the Remediation to *Enforce*
10. Select *Step 5* review that is under Remediation is set to *Enforce*
11. Click *Submit*

[start=12]
12. Navigate to the Results screen, allow the remediation to complete, _it may take longer to enforce the policy._

image::images/acm-images/policy7.png[]



Now you have succesfully created a Policy to scan your clusters, if you would like to play with other policies please visit the https://github.com/stolostron/policy-collection[Policy Repo] for more Policies you can test out.
